{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    This is a class that generates a fully-connected neural network.\n",
    "\n",
    "    Parameters:\n",
    "        nn_arch: List[Dict[str, float]]\n",
    "            A list of dictionaries describing the layers of the neural network.\n",
    "            e.g. [{'input_dim': 64, 'output_dim': 32, 'activation': 'relu'}, {'input_dim': 32, 'output_dim': 8, 'activation:': 'sigmoid'}]\n",
    "            will generate a two-layer deep network with an input dimension of 64, a 32 dimension hidden layer, and an 8 dimensional output.\n",
    "        lr: float\n",
    "            Learning rate (alpha).\n",
    "        seed: int\n",
    "            Random seed to ensure reproducibility.\n",
    "        batch_size: int\n",
    "            Size of mini-batches used for training.\n",
    "        epochs: int\n",
    "            Max number of epochs for training.\n",
    "        loss_function: str\n",
    "            Name of loss function.\n",
    "\n",
    "    Attributes:\n",
    "        arch: list of dicts\n",
    "            (see nn_arch above)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nn_arch: List[Dict[str, Union[int, str]]],\n",
    "        lr: float,\n",
    "        seed: int,\n",
    "        batch_size: int,\n",
    "        epochs: int,\n",
    "        loss_function: str\n",
    "    ):\n",
    "\n",
    "        # Save architecture\n",
    "        self.arch = nn_arch\n",
    "\n",
    "        # Save hyperparameters\n",
    "        self._lr = lr\n",
    "        self._seed = seed\n",
    "        self._epochs = epochs\n",
    "        self._loss_func = loss_function\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        # Initialize the parameter dictionary for use in training\n",
    "        self._param_dict = self._init_params()\n",
    "\n",
    "    def _init_params(self) -> Dict[str, ArrayLike]:\n",
    "        \"\"\"\n",
    "        DO NOT MODIFY THIS METHOD! IT IS ALREADY COMPLETE!\n",
    "\n",
    "        This method generates the parameter matrices for all layers of\n",
    "        the neural network. This function returns the param_dict after\n",
    "        initialization.\n",
    "\n",
    "        Returns:\n",
    "            param_dict: Dict[str, ArrayLike]\n",
    "                Dictionary of parameters in neural network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Seed NumPy\n",
    "        np.random.seed(self._seed)\n",
    "\n",
    "        # Define parameter dictionary\n",
    "        param_dict = {}\n",
    "\n",
    "        # Initialize each layer's weight matrices (W) and bias matrices (b)\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            layer_idx = idx + 1\n",
    "            input_dim = layer['input_dim']\n",
    "            output_dim = layer['output_dim']\n",
    "            #print(f\"Initializing parameters for layer {layer_idx}: input_dim={input_dim}, output_dim={output_dim}\")\n",
    "            param_dict['W' + str(layer_idx)] = np.random.randn(output_dim, input_dim) * 0.1\n",
    "            param_dict['b' + str(layer_idx)] = np.random.randn(output_dim, 1) * 0.1\n",
    "            #print(f\"W{layer_idx} shape: {param_dict['W' + str(layer_idx)].shape}\")\n",
    "            #print(f\"b{layer_idx} shape: {param_dict['b' + str(layer_idx)].shape}\")\n",
    "        return param_dict\n",
    "\n",
    "    def _single_forward(self, A_prev: ArrayLike, W_curr: ArrayLike, b_curr: ArrayLike, activation: str) -> Tuple[ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        Perform a single forward pass for one layer of the neural network.\n",
    "\n",
    "        :param A_prev: Input activations from the previous layer.\n",
    "        :param W_curr: Weights for the current layer.\n",
    "        :param b_curr: Biases for the current layer.\n",
    "        :param activation: Activation function to be used for the current layer.\n",
    "        :return: A tuple containing the activations and pre-activations (Z) of the current layer.\n",
    "        \"\"\"\n",
    "        expected_input_dim = W_curr.shape[1]\n",
    "        assert A_prev.shape[0] == expected_input_dim, f\"Shape of A_prev {A_prev.shape} does not match expected input_dim {expected_input_dim}\"\n",
    "        Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "\n",
    "        if activation == \"sigmoid\":\n",
    "            A_curr = self._sigmoid(Z_curr)\n",
    "        elif activation == \"relu\":\n",
    "            A_curr = self._relu(Z_curr)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "        return A_curr, Z_curr\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, X: ArrayLike) -> Tuple[ArrayLike, Dict[str, ArrayLike]]:\n",
    "        cache = {\"X\": X}\n",
    "        A_curr = X\n",
    "\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            A_prev = A_curr\n",
    "            W_curr = self._param_dict[\"W\" + str(idx + 1)]\n",
    "            b_curr = self._param_dict[\"b\" + str(idx + 1)]\n",
    "            activation = layer[\"activation\"] \n",
    "            A_curr, Z_curr = self._single_forward(A_prev, W_curr, b_curr, activation)\n",
    "            cache[\"A\" + str(idx + 1)] = A_curr\n",
    "            cache[\"Z\" + str(idx + 1)] = Z_curr\n",
    "\n",
    "        return A_curr.T, cache\n",
    "\n",
    "\n",
    "    def _single_backprop(\n",
    "    self,\n",
    "    dA_curr: ArrayLike,\n",
    "    W_curr: ArrayLike,\n",
    "    Z_curr: ArrayLike,\n",
    "    A_prev: ArrayLike,\n",
    "    activation_curr: str\n",
    "    ) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:\n",
    "\n",
    "        if activation_curr == 'sigmoid':\n",
    "            dZ_curr = self._sigmoid_backprop(dA_curr, Z_curr)\n",
    "        elif activation_curr == 'relu':\n",
    "            dZ_curr = self._relu_backprop(dA_curr, Z_curr)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation_curr}\")\n",
    "\n",
    "        m = A_prev.shape[1]\n",
    "        dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "        db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "        dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "        return dA_prev, dW_curr, db_curr\n",
    "    \n",
    "    def backprop(self, y, y_hat, cache):\n",
    "        grad_dict = {}\n",
    "\n",
    "        if self._loss_func == \"binary_cross_entropy\":\n",
    "            dA_prev = self._binary_cross_entropy_backprop(y, y_hat)\n",
    "        elif self._loss_func == \"mean_squared_error\":\n",
    "            dA_prev = self._mean_squared_error_backprop(y, y_hat)\n",
    "\n",
    "        for layer_idx_prev, layer in reversed(list(enumerate(self.arch))):\n",
    "            layer_idx_curr = layer_idx_prev + 1\n",
    "            activation_curr = layer[\"activation\"]\n",
    "\n",
    "            dA_curr = dA_prev\n",
    "\n",
    "            if layer_idx_prev == 0:  # Use X as A0\n",
    "                A_prev = cache[\"X\"]\n",
    "            else:\n",
    "                A_prev = cache[\"A\" + str(layer_idx_prev)]\n",
    "            Z_curr = cache[\"Z\" + str(layer_idx_curr)]\n",
    "\n",
    "            W_curr = self._param_dict[\"W\" + str(layer_idx_curr)]\n",
    "\n",
    "            dA_prev, dW_curr, db_curr = self._single_backprop(\n",
    "                dA_curr, W_curr, Z_curr, A_prev, activation_curr\n",
    "            )\n",
    "\n",
    "            grad_dict[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "            grad_dict[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "\n",
    "        return grad_dict\n",
    "\n",
    "\n",
    "    def _update_params(self, grad_dict: Dict[str, ArrayLike]):\n",
    "        for layer_idx, layer in enumerate(self.arch):\n",
    "            layer_idx += 1\n",
    "            self._param_dict[f\"W{layer_idx}\"] -= self._lr * grad_dict[f\"dW{layer_idx}\"]\n",
    "            self._param_dict[f\"b{layer_idx}\"] -= self._lr * grad_dict[f\"db{layer_idx}\"]\n",
    "\n",
    "    def _transpose_input(self, X):\n",
    "        \"\"\"\n",
    "        Transpose the input data to match the internal representation of the\n",
    "        neural network.\n",
    "        \"\"\"\n",
    "        return X.T\n",
    "\n",
    "\n",
    "    def fit(self, X_train: ArrayLike, y_train: ArrayLike, X_val: ArrayLike, y_val: ArrayLike) -> Tuple[List[float], List[float]]:\n",
    "        np.random.seed(self._seed)\n",
    "        self._init_params()\n",
    "\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "\n",
    "        for _ in range(self._epochs):\n",
    "            mini_batches = self._get_mini_batches(X_train, y_train)\n",
    "            \n",
    "            for X_mini_batch, y_mini_batch in mini_batches:\n",
    "                X_mini_batch = X_mini_batch.T\n",
    "                y_mini_batch = y_mini_batch.T\n",
    "                y_hat, cache = self.forward(X_mini_batch)\n",
    "                grad_dict = self.backprop(y_mini_batch, y_hat, cache)\n",
    "                self._update_params(grad_dict)\n",
    "\n",
    "            y_hat_train, _ = self.forward(X_train.T)\n",
    "            y_hat_val, _ = self.forward(X_val.T)\n",
    "\n",
    "            if self._loss_func == \"binary_cross_entropy\":\n",
    "                train_loss = self._binary_cross_entropy(y_train.T, y_hat_train.T)\n",
    "                val_loss = self._binary_cross_entropy(y_val.T, y_hat_val.T)\n",
    "            elif self._loss_func == \"mean_squared_error\":\n",
    "                train_loss = self._mean_squared_error(y_train.T, y_hat_train.T)\n",
    "                val_loss = self._mean_squared_error(y_val.T, y_hat_val.T)\n",
    "\n",
    "            train_loss_history.append(train_loss)\n",
    "            val_loss_history.append(val_loss)\n",
    "\n",
    "        return train_loss_history, val_loss_history\n",
    "\n",
    "    def _get_mini_batches(self, X, y):\n",
    "        \"\"\"\n",
    "        Generate mini-batches from the input data X and target data y.\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "        indices = np.arange(X.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        mini_batches = []\n",
    "        for idx in range(0, X.shape[0], self._batch_size):\n",
    "            batch_indices = indices[idx:idx+self._batch_size]\n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "            mini_batches.append((X_batch, y_batch))\n",
    "\n",
    "        return mini_batches\n",
    "\n",
    "    def predict(self, X: ArrayLike) -> ArrayLike:\n",
    "        y_hat, _ = self.forward(X.T)\n",
    "        return y_hat\n",
    "\n",
    "    \n",
    "    def _sigmoid(self, Z: ArrayLike) -> ArrayLike:\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    def _sigmoid_backprop(self, dA: ArrayLike, Z: ArrayLike):\n",
    "        s = self._sigmoid(Z)\n",
    "        return dA * s * (1 - s)\n",
    "\n",
    "    def _relu(self, Z: ArrayLike) -> ArrayLike:\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def _relu_backprop(self, dA: ArrayLike, Z: ArrayLike) -> ArrayLike:\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[Z <= 0] = 0\n",
    "        return dZ\n",
    "\n",
    "    def _binary_cross_entropy(self, y: ArrayLike, y_hat: ArrayLike) -> float:\n",
    "        m = y.shape[1]\n",
    "        loss = -(1 / m) * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "        return loss\n",
    "\n",
    "    def _binary_cross_entropy_backprop(self, y: ArrayLike, y_hat: ArrayLike) -> ArrayLike:\n",
    "        dA = - (np.divide(y, y_hat) - np.divide(1 - y, 1 - y_hat))\n",
    "        return dA\n",
    "\n",
    "    def _mean_squared_error(self, y: ArrayLike, y_hat: ArrayLike) -> float:\n",
    "        m = y.shape[1]\n",
    "        loss = (1 / (2 * m)) * np.sum(np.square(y_hat - y))\n",
    "        return loss\n",
    "\n",
    "    def _mean_squared_error_backprop(self, y: ArrayLike, y_hat: ArrayLike) -> ArrayLike:\n",
    "        m = y.shape[1]\n",
    "        dA = (2 / m) * (y_hat.T - y)\n",
    "        return dA\n",
    "    \n",
    "    def get_hyperparameters(self) -> dict:\n",
    "        \"\"\"\n",
    "        Get the current hyperparameters of the neural network.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing the hyperparameters.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'architecture': self.arch,\n",
    "            'learning_rate': self._lr,\n",
    "            'seed': self._seed,\n",
    "            'batch_size': self._batch_size,\n",
    "            'epochs': self._epochs,\n",
    "            'loss_function': self._loss_func\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reconstruction Error (Validation Set): 0.06253403640366616\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#custom function for splitting data into training and and testing subsets\n",
    "def train_test_split_custom(X, y, test_size=0.2, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    test_set_size = int(X.shape[0] * test_size)\n",
    "    test_indices = indices[:test_set_size]\n",
    "    train_indices = indices[test_set_size:]\n",
    "    \n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Normalize the data\n",
    "X /= 16\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split_custom(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Generate an instance of NeuralNetwork class with 64x16x64 autoencoder architecture\n",
    "nn_arch = [\n",
    "    {\"input_dim\": 64, \"output_dim\": 16, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 16, \"output_dim\": 64, \"activation\": \"sigmoid\"}\n",
    "]\n",
    "\n",
    "# Hyperparameter selection\n",
    "lr = 0.01\n",
    "seed = 42\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "loss_function = \"mean_squared_error\"\n",
    "\n",
    "# Create NeuralNetwork instance\n",
    "nn = NeuralNetwork(nn_arch, lr, seed, batch_size, epochs, loss_function)\n",
    "\n",
    "# Train the autoencoder\n",
    "train_loss, val_loss = nn.fit(X_train, X_train, X_val, X_val)\n",
    "\n",
    "# Plot training and validation loss by epoch\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Quantify average reconstruction error over the validation set\n",
    "y_val_pred = nn.predict(X_val)\n",
    "reconstruction_error = np.mean((y_val_pred - X_val) ** 2)\n",
    "print(\"Average Reconstruction Error (Validation Set):\", reconstruction_error)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation for hyperparameter choices:\n",
    "\n",
    "The architecture is a 64x16x64 autoencoder, where the input and output dimensions are 64, which matches the number of features in the dataset. The hidden layer has 16 neurons, which helps to learn a compressed representation of the input data.\n",
    "\n",
    "The number of epochs is set to 100, which is a reasonable choice for this dataset. It allows the model to have enough iterations to learn the underlying patterns while not being too computationally expensive.\n",
    "\n",
    "The batch size is set to 16, which is a common choice for mini-batch gradient descent. It strikes a balance between training speed and convergence.\n",
    "\n",
    "The learning rate is set to 0.01, which is a common choice for gradient descent optimization. It helps the model to learn at a reasonable pace without oscillating too much or getting stuck in local minima.\n",
    "\n",
    "Note that these hyperparameters can be further optimized through techniques like grid search, random search, or Bayesian optimization.\n",
    "\n",
    "Also Note that I've implementated both grid search and random search based hyperparameter optimiziation below which align reasonably well with the hyperparameter values that I've chosen here \n",
    "\n",
    "i.e.\n",
    "Grid-Search Results:\n",
    "Best Model Parameters: {'architecture': [{'input_dim': 64, 'output_dim': 32, 'activation': 'relu'}, {'input_dim': 32, 'output_dim': 64, 'activation': 'sigmoid'}], 'learning_rate': 0.1, 'seed': 42, 'batch_size': 16, 'epochs': 100, 'loss_function': 'mean_squared_error'}, Reconstruction Error: 0.013262797432556001\n",
    "\n",
    "Random-Search results:\n",
    "Best Model Parameters: {'architecture': [{'input_dim': 64, 'output_dim': 16, 'activation': 'relu'}, {'input_dim': 16, 'output_dim': 64, 'activation': 'sigmoid'}], 'learning_rate': 0.1, 'seed': 42, 'batch_size': 16, 'epochs': 100, 'loss_function': 'mean_squared_error'}, Reconstruction Error: 0.018418971196789927\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid search based hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'hidden_neurons': [8, 16, 32]\n",
    "}\n",
    "\n",
    "# Create a ParameterGrid instance\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "# Store the best model and its reconstruction error\n",
    "best_model = None\n",
    "best_reconstruction_error = float('inf')\n",
    "\n",
    "# Perform Grid Search\n",
    "for params in grid:\n",
    "    # Update the architecture with the current hidden_neurons value\n",
    "    nn_arch[0]['output_dim'] = params['hidden_neurons']\n",
    "    nn_arch[1]['input_dim'] = params['hidden_neurons']\n",
    "\n",
    "    # Create NeuralNetwork instance with the current hyperparameters\n",
    "    nn = NeuralNetwork(nn_arch, params['learning_rate'], seed, params['batch_size'], epochs, loss_function)\n",
    "\n",
    "    # Train the autoencoder\n",
    "    train_loss, val_loss = nn.fit(X_train, X_train, X_val, X_val)\n",
    "\n",
    "    # Compute average reconstruction error over the validation set\n",
    "    y_val_pred = nn.predict(X_val)\n",
    "    reconstruction_error = np.mean((y_val_pred - X_val) ** 2)\n",
    "\n",
    "    # Update the best model if the current model has lower reconstruction error\n",
    "    if reconstruction_error < best_reconstruction_error:\n",
    "        best_reconstruction_error = reconstruction_error\n",
    "        best_model = nn\n",
    "\n",
    "    print(f\"Parameters: {params}, Reconstruction Error: {reconstruction_error}\")\n",
    "\n",
    "print(f\"Best Model Parameters: {best_model.get_hyperparameters()}, Reconstruction Error: {best_reconstruction_error}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search based hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Normalize the data\n",
    "X /= 16\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fixed autoencoder architecture\n",
    "nn_arch = [\n",
    "    {\"input_dim\": 64, \"output_dim\": 16, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 16, \"output_dim\": 64, \"activation\": \"sigmoid\"}\n",
    "]\n",
    "\n",
    "# Random search hyperparameter ranges\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [16, 32, 64]\n",
    "epochs = [50, 100, 200]\n",
    "loss_functions = [\"mean_squared_error\"]\n",
    "n_search_iter = 10\n",
    "\n",
    "best_reconstruction_error = float('inf')\n",
    "best_model = None\n",
    "\n",
    "for _ in range(n_search_iter):\n",
    "    # Randomly select hyperparameters\n",
    "    lr = random.choice(learning_rates)\n",
    "    batch_size = random.choice(batch_sizes)\n",
    "    epoch = random.choice(epochs)\n",
    "    loss_function = random.choice(loss_functions)\n",
    "\n",
    "    # Create a NeuralNetwork instance\n",
    "    nn = NeuralNetwork(nn_arch, lr, seed, batch_size, epoch, loss_function)\n",
    "\n",
    "    # Train the autoencoder\n",
    "    train_loss, val_loss = nn.fit(X_train, X_train, X_val, X_val)\n",
    "\n",
    "    # Calculate average reconstruction error over the validation set\n",
    "    y_val_pred = nn.predict(X_val)\n",
    "    reconstruction_error = np.mean((y_val_pred - X_val) ** 2)\n",
    "\n",
    "    # Update best model and reconstruction error\n",
    "    if reconstruction_error < best_reconstruction_error:\n",
    "        best_reconstruction_error = reconstruction_error\n",
    "        best_model = nn\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': lr,\n",
    "        'batch_size': batch_size,\n",
    "        'epochs': epoch,\n",
    "        'loss_function': loss_function\n",
    "    }\n",
    "    print(f\"Parameters: {params}, Reconstruction Error: {reconstruction_error}\")\n",
    "\n",
    "print(f\"Best Model Parameters: {best_model.get_hyperparameters()}, Reconstruction Error: {best_reconstruction_error}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
