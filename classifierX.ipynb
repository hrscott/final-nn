{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/henryrscott/Desktop/Algos/final/final-nn/classifierX.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/classifierX.ipynb#W1sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m rap1_nn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mNeuralNetwork(nn_arch, lr, seed, batch_size, epochs, loss_function)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/classifierX.ipynb#W1sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Train the neural network\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/classifierX.ipynb#W1sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m train_losses, val_losses \u001b[39m=\u001b[39m rap1_nn\u001b[39m.\u001b[39;49mfit(X_train, y_train, X_val, y_val)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/classifierX.ipynb#W1sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# 7. Train the neural network on the training data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/classifierX.ipynb#W1sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m#train_losses, val_losses = rap1_nn.fit(X_train, np.array(y_train)[:, np.newaxis], X_val, np.array(y_val)[:, np.newaxis])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/classifierX.ipynb#W1sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/classifierX.ipynb#W1sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# 8. Plot training and validation loss by epoch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/classifierX.ipynb#W1sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(train_losses, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTraining loss\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Algos/final/final-nn/nn/nn.py:205\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    202\u001b[0m val_loss_history \u001b[39m=\u001b[39m []\n\u001b[1;32m    204\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_epochs):\n\u001b[0;32m--> 205\u001b[0m     mini_batches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_mini_batches(X_train, y_train)\n\u001b[1;32m    207\u001b[0m     \u001b[39mfor\u001b[39;00m X_mini_batch, y_mini_batch \u001b[39min\u001b[39;00m mini_batches:\n\u001b[1;32m    208\u001b[0m         X_mini_batch \u001b[39m=\u001b[39m X_mini_batch\u001b[39m.\u001b[39mT\n",
      "File \u001b[0;32m~/Desktop/Algos/final/final-nn/nn/nn.py:235\u001b[0m, in \u001b[0;36mNeuralNetwork._get_mini_batches\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_mini_batches\u001b[39m(\u001b[39mself\u001b[39m, X, y):\n\u001b[1;32m    232\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39m#    Generate mini-batches from the input data X and target data y.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[39m#     \"\"\"\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m     \u001b[39massert\u001b[39;00m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    236\u001b[0m     indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m    237\u001b[0m     np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mshuffle(indices)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nn import io, preprocess, nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Read positive and negative sequences\n",
    "\n",
    "positive_seqs = 'data/rap1-lieb-positives.txt'\n",
    "negative_seqs = 'data/yeast-upstream-1k-negative.fa'\n",
    "\n",
    "\n",
    "pos_seqs = io.read_text_file(positive_seqs)\n",
    "neg_seqs = io.read_fasta_file(negative_seqs)\n",
    "\n",
    "# 2. Process the negative sequences to match the length of the positive sequences\n",
    "processed_neg_seqs = []\n",
    "for seq in neg_seqs:\n",
    "    for i in range(len(seq) - 17 + 1):\n",
    "        subseq = seq[i:i + 17]\n",
    "        processed_neg_seqs.append(subseq)\n",
    "\n",
    "# 3. Balance the classes using the sample_seqs function\n",
    "all_seqs = pos_seqs + processed_neg_seqs\n",
    "all_labels = [True] * len(pos_seqs) + [False] * len(processed_neg_seqs)\n",
    "sampled_seqs, sampled_labels = preprocess.sample_seqs(all_seqs, all_labels)\n",
    "\n",
    "# The chosen sampling scheme ensures that both classes have an equal number of samples,\n",
    "# which helps to prevent the model from being biased towards the majority class.\n",
    "\n",
    "# 4. One-hot encode the data\n",
    "encoded_seqs = preprocess.one_hot_encode_seqs(sampled_seqs)\n",
    "\n",
    "# 5. Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(encoded_seqs, sampled_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure that X_train and X_val have shape (number_of_features, number_of_samples)\n",
    "X_train = X_train.T\n",
    "X_val = X_val.T\n",
    "\n",
    "# Ensure that y_train and y_val have shape (number_of_samples, 1) instead of (1, number_of_samples)\n",
    "y_train = np.array(y_train).reshape(-1, 1)\n",
    "y_val = np.array(y_val).reshape(-1, 1)\n",
    "\n",
    "# 6. Create an instance of the NeuralNetwork class with an appropriate architecture\n",
    "nn_arch = [\n",
    "    {'input_dim': 68, 'output_dim': 32, 'activation': 'relu'},\n",
    "    {'input_dim': 32, 'output_dim': 1, 'activation': 'sigmoid'}\n",
    "]\n",
    "lr = 0.01\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "loss_function = \"binary_cross_entropy\"\n",
    "\n",
    "rap1_nn = nn.NeuralNetwork(nn_arch, lr, seed, batch_size, epochs, loss_function)\n",
    "\n",
    "# Train the neural network\n",
    "train_losses, val_losses = rap1_nn.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# 7. Train the neural network on the training data\n",
    "#train_losses, val_losses = rap1_nn.fit(X_train, np.array(y_train)[:, np.newaxis], X_val, np.array(y_val)[:, np.newaxis])\n",
    "\n",
    "# 8. Plot training and validation loss by epoch\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 9. Report the accuracy of the classifier on the validation dataset\n",
    "y_val_pred = rap1_nn.predict(X_val)\n",
    "accuracy = np.mean((y_val_pred > 0.5) == np.array(y_val)[:, np.newaxis])\n",
    "print(f'Validation accuracy: {accuracy}')\n",
    "\n",
    "# Explanation for the choice of loss function and hyperparameters:\n",
    "# Loss function: Binary cross-entropy is used because it's a standard loss function for binary classification problems.\n",
    "# It measures the dissimilarity between the predicted probabilities and the true labels.\n",
    "# Hyperparameters: The learning rate, batch size, and epochs are chosen based on common practice and can be further fine-tuned\n",
    "# using grid search or other hyperparameter optimization techniques to improve the performance of the model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
