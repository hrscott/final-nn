{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    This is a class that generates a fully-connected neural network.\n",
    "\n",
    "    Parameters:\n",
    "        nn_arch: List[Dict[str, float]]\n",
    "            A list of dictionaries describing the layers of the neural network.\n",
    "            e.g. [{'input_dim': 64, 'output_dim': 32, 'activation': 'relu'}, {'input_dim': 32, 'output_dim': 8, 'activation:': 'sigmoid'}]\n",
    "            will generate a two-layer deep network with an input dimension of 64, a 32 dimension hidden layer, and an 8 dimensional output.\n",
    "        lr: float\n",
    "            Learning rate (alpha).\n",
    "        seed: int\n",
    "            Random seed to ensure reproducibility.\n",
    "        batch_size: int\n",
    "            Size of mini-batches used for training.\n",
    "        epochs: int\n",
    "            Max number of epochs for training.\n",
    "        loss_function: str\n",
    "            Name of loss function.\n",
    "\n",
    "    Attributes:\n",
    "        arch: list of dicts\n",
    "            (see nn_arch above)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nn_arch: List[Dict[str, Union[int, str]]],\n",
    "        lr: float,\n",
    "        seed: int,\n",
    "        batch_size: int,\n",
    "        epochs: int,\n",
    "        loss_function: str\n",
    "    ):\n",
    "\n",
    "        # Save architecture\n",
    "        self.arch = nn_arch\n",
    "\n",
    "        # Save hyperparameters\n",
    "        self._lr = lr\n",
    "        self._seed = seed\n",
    "        self._epochs = epochs\n",
    "        self._loss_func = loss_function\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        # Initialize the parameter dictionary for use in training\n",
    "        self._param_dict = self._init_params()\n",
    "\n",
    "    def _init_params(self) -> Dict[str, ArrayLike]:\n",
    "        \"\"\"\n",
    "        DO NOT MODIFY THIS METHOD! IT IS ALREADY COMPLETE!\n",
    "\n",
    "        This method generates the parameter matrices for all layers of\n",
    "        the neural network. This function returns the param_dict after\n",
    "        initialization.\n",
    "\n",
    "        Returns:\n",
    "            param_dict: Dict[str, ArrayLike]\n",
    "                Dictionary of parameters in neural network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Seed NumPy\n",
    "        np.random.seed(self._seed)\n",
    "\n",
    "        # Define parameter dictionary\n",
    "        param_dict = {}\n",
    "\n",
    "        # Initialize each layer's weight matrices (W) and bias matrices (b)\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            layer_idx = idx + 1\n",
    "            input_dim = layer['input_dim']\n",
    "            output_dim = layer['output_dim']\n",
    "            param_dict['W' + str(layer_idx)] = np.random.randn(output_dim, input_dim) * 0.1\n",
    "            param_dict['b' + str(layer_idx)] = np.random.randn(output_dim, 1) * 0.1\n",
    "\n",
    "        return param_dict\n",
    "\n",
    "    def _single_forward(\n",
    "        self,\n",
    "        W_curr: ArrayLike,\n",
    "        b_curr: ArrayLike,\n",
    "        A_prev: ArrayLike,\n",
    "        activation: str\n",
    "    ) -> Tuple[ArrayLike, ArrayLike]:\n",
    "            \"\"\"\n",
    "            This method is used for a single forward pass on a single layer.\n",
    "\n",
    "            Args:\n",
    "                W: ArrayLike\n",
    "                    Current layer weight matrix.\n",
    "                b: ArrayLike\n",
    "                    Current layer bias matrix.\n",
    "                A_prev: ArrayLike\n",
    "                    Previous layer activation matrix.\n",
    "                activation: str\n",
    "                    Name of activation function for current layer.\n",
    "\n",
    "            Returns:\n",
    "                A_curr: ArrayLike\n",
    "                    Current layer activation matrix.\n",
    "                Z_curr: ArrayLike\n",
    "                    Current layer linear transformed matrix.\n",
    "            \"\"\"\n",
    "            # print(f\"W_curr: {W_curr.shape}\")\n",
    "            # print(f\"b_curr: {b_curr.shape}\")\n",
    "            # print(f\"A_prev: {A_prev.shape}\")\n",
    "            Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "            if activation == 'relu':\n",
    "                A_curr = self._relu(Z_curr)\n",
    "            elif activation == 'sigmoid':\n",
    "                A_curr = self._sigmoid(Z_curr)\n",
    "            else:\n",
    "                raise ValueError(f\"{activation} is not a valid activation function.\")\n",
    "            # print(f\"A_curr: {A_curr.shape}\")\n",
    "            # print(f\"Z_curr: {Z_curr.shape}\")\n",
    "            return A_curr, Z_curr\n",
    "\n",
    "\n",
    "    def forward(self, X: ArrayLike) -> Tuple[ArrayLike, Dict[str, ArrayLike]]:\n",
    "        \"\"\"\n",
    "        This method is responsible for one forward pass of the entire neural network.\n",
    "\n",
    "        Args:\n",
    "            X: ArrayLike\n",
    "                Input matrix with shape [batch_size, features].\n",
    "\n",
    "        Returns:\n",
    "            output: ArrayLike\n",
    "                Output of forward pass.\n",
    "            cache: Dict[str, ArrayLike]:\n",
    "                Dictionary storing Z and A matrices from `_single_forward` for use in backprop.\n",
    "        \"\"\"\n",
    "        # Initialize cache dictionary\n",
    "        cache = {}\n",
    "\n",
    "        # Compute the activation matrix of the input layer\n",
    "        A_curr = X.T\n",
    "\n",
    "        # Loop through each layer of the neural network\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            # Increment layer index\n",
    "            layer_idx = idx + 1\n",
    "\n",
    "            # Get the weight matrix, bias matrix and activation function for the current layer\n",
    "            W_curr = self._param_dict[f'W{layer_idx}']\n",
    "            b_curr = self._param_dict[f'b{layer_idx}']\n",
    "            activation = layer['activation']\n",
    "\n",
    "            # Compute activation and linear transformation matrices for current layer\n",
    "            A_curr, Z_curr = self._single_forward(W_curr, b_curr, A_curr, activation)\n",
    "\n",
    "            # Store A and Z matrices in cache dictionary\n",
    "            cache[f\"A{layer_idx}\"] = A_curr\n",
    "            cache[f\"Z{layer_idx}\"] = Z_curr\n",
    "\n",
    "        # Return final output and cache dictionary\n",
    "        output = A_curr.T\n",
    "        return output, cache\n",
    "\n",
    "    def _single_backprop(\n",
    "        self,\n",
    "        W_curr: ArrayLike,\n",
    "        b_curr: ArrayLike,\n",
    "        Z_curr: ArrayLike,\n",
    "        A_prev: ArrayLike,\n",
    "        dA_curr: ArrayLike,\n",
    "        activation: str\n",
    "    ) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        This function performs the backprop for a single layer in the network.\n",
    "\n",
    "        Args:\n",
    "            W_curr: ArrayLike\n",
    "                Weight matrix for the current layer.\n",
    "            b_curr: ArrayLike\n",
    "                Bias matrix for the current layer.\n",
    "            Z_curr: ArrayLike\n",
    "                Output matrix of the linear transform for the current layer.\n",
    "            A_prev: ArrayLike\n",
    "                Activation matrix from the previous layer.\n",
    "            dA_curr: ArrayLike\n",
    "                Partial derivative of current layer activations.\n",
    "            activation: str\n",
    "                String indicating the activation function to use.\n",
    "\n",
    "        Returns:\n",
    "            dA_prev: ArrayLike\n",
    "                Partial derivative of the previous layer's activations.\n",
    "            dW_curr: ArrayLike\n",
    "                Partial derivative of the current layer's weight matrix.\n",
    "            db_curr: ArrayLike\n",
    "                Partial derivative of the current layer's bias matrix.\n",
    "        \"\"\"\n",
    "        m = A_prev.shape[1]\n",
    "\n",
    "        if activation == 'sigmoid':\n",
    "            dZ_curr = dA_curr * self._sigmoid_backprop(dA_curr, Z_curr)\n",
    "        elif activation == 'relu':\n",
    "            dZ_curr = dA_curr * self._relu_backprop(dA_curr, Z_curr)\n",
    "\n",
    "        dW_curr = 1/m * np.dot(dZ_curr, A_prev.T).T  # Transposed here\n",
    "        db_curr = 1/m * np.sum(dZ_curr, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "        return dA_prev, dW_curr, db_curr\n",
    "\n",
    "\n",
    "    def backprop(self, y: ArrayLike, y_hat: ArrayLike, cache: Dict[str, ArrayLike]):\n",
    "        \"\"\"\n",
    "        This method is responsible for the backprop of the whole fully connected neural network.\n",
    "\n",
    "        Args:\n",
    "            y (array-like):\n",
    "                Ground truth labels.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output values.\n",
    "            cache: Dict[str, ArrayLike]\n",
    "                Dictionary containing the information about the\n",
    "                most recent forward pass, specifically A and Z matrices.\n",
    "\n",
    "        Returns:\n",
    "            grad_dict: Dict[str, ArrayLike]\n",
    "                Dictionary containing the gradient information from this pass of backprop.\n",
    "        \"\"\"\n",
    "        # Initialize dictionary to store gradients\n",
    "        grad_dict = {}\n",
    "\n",
    "        # Compute derivative of loss with respect to output layer activations\n",
    "        if self._loss_func == 'binary_cross_entropy':\n",
    "            dA_curr = self._binary_cross_entropy_backprop(y, y_hat)\n",
    "        elif self._loss_func == 'mse':\n",
    "            dA_curr = self._mean_squared_error_backprop(y, y_hat)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported loss function.\")\n",
    "\n",
    "        # Loop through layers backwards\n",
    "        for layer_idx, layer in reversed(list(enumerate(self.arch))):\n",
    "            layer_idx = layer_idx + 1\n",
    "\n",
    "            # Get relevant parameter values\n",
    "            A_prev = cache['A' + str(layer_idx - 1)]\n",
    "            W_curr = self._param_dict['W' + str(layer_idx)]\n",
    "            b_curr = self._param_dict['b' + str(layer_idx)]\n",
    "            Z_curr = cache['Z' + str(layer_idx)]\n",
    "\n",
    "            # Compute gradients for current layer\n",
    "            dA_prev, dW_curr, db_curr = self._single_backprop(W_curr, b_curr, Z_curr, A_prev, dA_curr, layer['activation'])\n",
    "\n",
    "            # Save gradients for current layer\n",
    "            grad_dict['dW' + str(layer_idx)] = dW_curr\n",
    "            grad_dict['db' + str(layer_idx)] = db_curr\n",
    "\n",
    "            # Set dA_curr for next iteration to be dA_prev\n",
    "            dA_curr = dA_prev\n",
    "\n",
    "        return grad_dict\n",
    "\n",
    "    def _update_params(self, grad_dict: Dict[str, ArrayLike]):\n",
    "        \"\"\"\n",
    "        This function updates the parameters in the neural network after backprop. This function\n",
    "        only modifies internal attributes and does not return anything\n",
    "\n",
    "        Args:\n",
    "            grad_dict: Dict[str, ArrayLike]\n",
    "                Dictionary containing the gradient information from most recent round of backprop.\n",
    "        \"\"\"\n",
    "        for key in self._param_dict:\n",
    "            if 'W' in key:\n",
    "                self._param_dict[key] -= self._lr * grad_dict['d' + key]\n",
    "            elif 'b' in key:\n",
    "                self._param_dict[key] -= self._lr * grad_dict['d' + key]\n",
    "\n",
    "\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X_train: ArrayLike,\n",
    "        y_train: ArrayLike,\n",
    "        X_val: ArrayLike,\n",
    "        y_val: ArrayLike\n",
    "    ) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        This function trains the neural network by backpropagation for the number of epochs defined at\n",
    "        the initialization of this class instance.\n",
    "\n",
    "        Args:\n",
    "            X_train: ArrayLike\n",
    "                Input features of training set.\n",
    "            y_train: ArrayLike\n",
    "                Labels for training set.\n",
    "            X_val: ArrayLike\n",
    "                Input features of validation set.\n",
    "            y_val: ArrayLike\n",
    "                Labels for validation set.\n",
    "\n",
    "        Returns:\n",
    "            per_epoch_loss_train: List[float]\n",
    "                List of per epoch loss for training set.\n",
    "            per_epoch_loss_val: List[float]\n",
    "                List of per epoch loss for validation set.\n",
    "        \"\"\"\n",
    "        num_train_samples = X_train.shape[0]\n",
    "        num_val_samples = X_val.shape[0]\n",
    "\n",
    "        per_epoch_loss_train = []\n",
    "        per_epoch_loss_val = []\n",
    "\n",
    "        for epoch in range(self._epochs):\n",
    "            # Shuffle the training data\n",
    "            permutation = np.random.permutation(num_train_samples)\n",
    "            X_train = X_train[permutation]\n",
    "            y_train = y_train[permutation]\n",
    "\n",
    "            # Initialize loss for epoch\n",
    "            epoch_loss_train = 0\n",
    "\n",
    "            # Train on mini-batches\n",
    "            for i in range(0, num_train_samples, self._batch_size):\n",
    "                X_batch = X_train[i:i + self._batch_size]\n",
    "                y_batch = y_train[i:i + self._batch_size]\n",
    "\n",
    "                # Forward pass\n",
    "                y_hat_batch, cache = self.forward(X_batch)\n",
    "\n",
    "                # Compute loss\n",
    "                loss_batch = self._binary_cross_entropy(y_batch, y_hat_batch)\n",
    "                epoch_loss_train += loss_batch\n",
    "\n",
    "                # Backward pass\n",
    "                grad_dict = self.backprop(y_batch, y_hat_batch, cache)\n",
    "\n",
    "                # Update parameters\n",
    "                self._update_params(grad_dict)\n",
    "\n",
    "            # Compute per-epoch loss and add to list\n",
    "            epoch_loss_train /= num_train_samples\n",
    "            per_epoch_loss_train.append(epoch_loss_train)\n",
    "\n",
    "            # Compute validation loss\n",
    "            y_hat_val, _ = self.forward(X_val)\n",
    "            epoch_loss_val = self._binary_cross_entropy(y_val, y_hat_val)\n",
    "            per_epoch_loss_val.append(epoch_loss_val)\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch + 1}: train_loss={epoch_loss_train:.4f}, val_loss={epoch_loss_val:.4f}\")\n",
    "\n",
    "        return per_epoch_loss_train, per_epoch_loss_val\n",
    "\n",
    "    \n",
    "\n",
    "    def predict(self, X: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        This function returns the prediction of the neural network.\n",
    "\n",
    "        Args:\n",
    "            X: ArrayLike\n",
    "                Input data for prediction.\n",
    "\n",
    "        Returns:\n",
    "            y_hat: ArrayLike\n",
    "                Prediction from the model.\n",
    "        \"\"\"\n",
    "        # Do a forward pass\n",
    "        output, _ = self.forward(X)\n",
    "\n",
    "        # Get predictions (assuming binary classification)\n",
    "        y_hat = np.round(output)\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "    def _sigmoid(self, Z: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "\n",
    "        Args:\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            nl_transform: ArrayLike\n",
    "                Activation function output.\n",
    "        \"\"\"\n",
    "        nl_transform = 1 / (1 + np.exp(-Z))\n",
    "        return nl_transform\n",
    "    \n",
    "\n",
    "    def _sigmoid_backprop(self, dA: ArrayLike, Z: ArrayLike):\n",
    "        \"\"\"\n",
    "        Sigmoid derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            dA: ArrayLike\n",
    "                Partial derivative of previous layer activation matrix.\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            dZ: ArrayLike\n",
    "                Partial derivative of current layer Z matrix.\n",
    "        \"\"\"\n",
    "        sigmoid = self._sigmoid(Z)\n",
    "\n",
    "        # print(f\"dA shape: {dA.shape}\")\n",
    "        # print(f\"sigmoid shape: {sigmoid.shape}\")\n",
    "        # print(f\"(1 - sigmoid) shape: {(1 - sigmoid).shape}\")\n",
    "        \n",
    "        dZ = dA * sigmoid * (1 - sigmoid)\n",
    "        return dZ\n",
    "\n",
    "\n",
    "    def _relu(self, Z: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        ReLU activation function.\n",
    "\n",
    "        Args:\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            nl_transform: ArrayLike\n",
    "                Activation function output.\n",
    "        \"\"\"\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def _relu_backprop(self, dA: ArrayLike, Z: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        ReLU derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            dA: ArrayLike\n",
    "                Partial derivative of previous layer activation matrix.\n",
    "            Z: ArrayLike\n",
    "                Output of layer linear transform.\n",
    "\n",
    "        Returns:\n",
    "            dZ: ArrayLike\n",
    "                Partial derivative of current layer Z matrix.\n",
    "        \"\"\"\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[Z <= 0] = 0\n",
    "        return dZ\n",
    "    \n",
    "    def _binary_cross_entropy(self, y: ArrayLike, y_hat: ArrayLike) -> float:\n",
    "        \"\"\"\n",
    "        Binary cross entropy loss function.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss over mini-batch.\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        loss = -1/m * np.sum(y * np.log(y_hat) + (1-y) * np.log(1-y_hat))\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def _binary_cross_entropy_backprop(self, y: ArrayLike, y_hat: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Binary cross entropy loss function derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            dA: ArrayLike\n",
    "                partial derivative of loss with respect to A matrix.\n",
    "        \"\"\"\n",
    "        return -(np.divide(y, y_hat) - np.divide(1 - y, 1 - y_hat))\n",
    "\n",
    "    def _mean_squared_error(self, y: ArrayLike, y_hat: ArrayLike) -> float:\n",
    "        \"\"\"\n",
    "        Mean squared error loss.\n",
    "\n",
    "        Args:\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "\n",
    "        Returns:\n",
    "            loss: float\n",
    "                Average loss of mini-batch.\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        loss = np.sum((y - y_hat) ** 2) / m\n",
    "        return loss\n",
    "\n",
    "    def _mean_squared_error_backprop(self, y: ArrayLike, y_hat: ArrayLike) -> ArrayLike:\n",
    "        \"\"\"\n",
    "        Mean square error loss derivative for backprop.\n",
    "\n",
    "        Args:\n",
    "            y_hat: ArrayLike\n",
    "                Predicted output.\n",
    "            y: ArrayLike\n",
    "                Ground truth output.\n",
    "\n",
    "        Returns:\n",
    "            dA: ArrayLike\n",
    "                partial derivative of loss with respect to A matrix.\n",
    "        \"\"\"\n",
    "        m = y.shape[0]\n",
    "        dA = -2 * (y - y_hat) / m\n",
    "        return dA\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "def sample_seqs(seqs: List[str], labels: List[bool]) -> Tuple[List[str], List[bool]]:\n",
    "    \"\"\"\n",
    "    This function should sample the given sequences to account for class imbalance. \n",
    "    Consider this a sampling scheme with replacement.\n",
    "    \n",
    "    Args:\n",
    "        seqs: List[str]\n",
    "            List of all sequences.\n",
    "        labels: List[bool]\n",
    "            List of positive/negative labels\n",
    "\n",
    "    Returns:\n",
    "        sampled_seqs: List[str]\n",
    "            List of sampled sequences which reflect a balanced class size\n",
    "        sampled_labels: List[bool]\n",
    "            List of labels for the sampled sequences\n",
    "    \"\"\"\n",
    "    pos_seqs = [seq for seq, label in zip(seqs, labels) if label]\n",
    "    neg_seqs = [seq for seq, label in zip(seqs, labels) if not label]\n",
    "    \n",
    "    # Calculate number of sequences to sample from each class\n",
    "    n_pos = len(pos_seqs)\n",
    "    n_neg = len(neg_seqs)\n",
    "    n_samples = min(n_pos, n_neg)\n",
    "    \n",
    "    # Sample sequences with replacement\n",
    "    pos_samples = np.random.choice(pos_seqs, n_samples, replace=True)\n",
    "    neg_samples = np.random.choice(neg_seqs, n_samples, replace=True)\n",
    "    \n",
    "    # Combine the sampled sequences and labels\n",
    "    sampled_seqs = list(pos_samples) + list(neg_samples)\n",
    "    sampled_labels = [True] * n_samples + [False] * n_samples\n",
    "    \n",
    "    # Shuffle the sequences and labels\n",
    "    shuffle_idx = np.random.permutation(len(sampled_seqs))\n",
    "    sampled_seqs = [sampled_seqs[i] for i in shuffle_idx]\n",
    "    sampled_labels = [sampled_labels[i] for i in shuffle_idx]\n",
    "    \n",
    "    return sampled_seqs, sampled_labels\n",
    "\n",
    "def one_hot_encode_seqs(seq_arr: List[str]) -> ArrayLike:\n",
    "    \"\"\"\n",
    "    This function generates a flattened one-hot encoding of a list of DNA sequences\n",
    "    for use as input into a neural network.\n",
    "\n",
    "    Args:\n",
    "        seq_arr: List[str]\n",
    "            List of sequences to encode.\n",
    "\n",
    "    Returns:\n",
    "        encodings: ArrayLike\n",
    "            Array of encoded sequences, with each encoding 4x as long as the input sequence.\n",
    "            For example, if we encode:\n",
    "                A -> [1, 0, 0, 0]\n",
    "                T -> [0, 1, 0, 0]\n",
    "                C -> [0, 0, 1, 0]\n",
    "                G -> [0, 0, 0, 1]\n",
    "            Then, AGA -> [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0].\n",
    "    \"\"\"\n",
    "    # Define the one-hot encoding dictionary\n",
    "    encoding_dict = {'A': [1, 0, 0, 0], 'T': [0, 1, 0, 0], 'C': [0, 0, 1, 0], 'G': [0, 0, 0, 1]}\n",
    "\n",
    "    # Initialize an empty list to store the encodings\n",
    "    encodings = []\n",
    "\n",
    "    # Iterate over each sequence in seq_arr\n",
    "    for seq in seq_arr:\n",
    "        # Initialize an empty list to store the one-hot encoding of this sequence\n",
    "        encoding = []\n",
    "        # Iterate over each nucleotide in the sequence\n",
    "        for nt in seq:\n",
    "            # Append the one-hot encoding of this nucleotide to the encoding list\n",
    "            encoding += encoding_dict[nt]\n",
    "        # Append the encoding to the list of encodings\n",
    "        encodings.append(encoding)\n",
    "\n",
    "    # Convert the list of encodings to a NumPy array and return it\n",
    "    return np.array(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1797, 64)\n",
      "Labels shape: (1797,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (32,64) (64,32) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Initialize and train the neural network\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m nn \u001b[39m=\u001b[39m NeuralNetwork(nn_arch\u001b[39m=\u001b[39m[{\u001b[39m'\u001b[39m\u001b[39minput_dim\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m64\u001b[39m, \u001b[39m'\u001b[39m\u001b[39moutput_dim\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m16\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mactivation\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m}, {\u001b[39m'\u001b[39m\u001b[39minput_dim\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m16\u001b[39m, \u001b[39m'\u001b[39m\u001b[39moutput_dim\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m64\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mactivation\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m}],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m                    lr\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, loss_function\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m nn\u001b[39m.\u001b[39;49mfit(X_train, X_train, X_val, X_val)\n",
      "\u001b[1;32m/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb Cell 3\u001b[0m in \u001b[0;36mNeuralNetwork.fit\u001b[0;34m(self, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=331'>332</a>\u001b[0m epoch_loss_train \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss_batch\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=333'>334</a>\u001b[0m \u001b[39m# Backward pass\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=334'>335</a>\u001b[0m grad_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackprop(y_batch, y_hat_batch, cache)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=336'>337</a>\u001b[0m \u001b[39m# Update parameters\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=337'>338</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_params(grad_dict)\n",
      "\u001b[1;32m/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb Cell 3\u001b[0m in \u001b[0;36mNeuralNetwork.backprop\u001b[0;34m(self, y, y_hat, cache)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=248'>249</a>\u001b[0m Z_curr \u001b[39m=\u001b[39m cache[\u001b[39m'\u001b[39m\u001b[39mZ\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(layer_idx)]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=250'>251</a>\u001b[0m \u001b[39m# Compute gradients for current layer\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=251'>252</a>\u001b[0m dA_prev, dW_curr, db_curr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_single_backprop(W_curr, b_curr, Z_curr, A_prev, dA_curr, layer[\u001b[39m'\u001b[39;49m\u001b[39mactivation\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=253'>254</a>\u001b[0m \u001b[39m# Save gradients for current layer\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=254'>255</a>\u001b[0m grad_dict[\u001b[39m'\u001b[39m\u001b[39mdW\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(layer_idx)] \u001b[39m=\u001b[39m dW_curr\n",
      "\u001b[1;32m/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb Cell 3\u001b[0m in \u001b[0;36mNeuralNetwork._single_backprop\u001b[0;34m(self, W_curr, b_curr, Z_curr, A_prev, dA_curr, activation)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=198'>199</a>\u001b[0m m \u001b[39m=\u001b[39m A_prev\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=200'>201</a>\u001b[0m \u001b[39mif\u001b[39;00m activation \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=201'>202</a>\u001b[0m     dZ_curr \u001b[39m=\u001b[39m dA_curr \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sigmoid_backprop(dA_curr, Z_curr)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=202'>203</a>\u001b[0m \u001b[39melif\u001b[39;00m activation \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=203'>204</a>\u001b[0m     dZ_curr \u001b[39m=\u001b[39m dA_curr \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_relu_backprop(dA_curr, Z_curr)\n",
      "\u001b[1;32m/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb Cell 3\u001b[0m in \u001b[0;36mNeuralNetwork._sigmoid_backprop\u001b[0;34m(self, dA, Z)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=406'>407</a>\u001b[0m sigmoid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sigmoid(Z)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=408'>409</a>\u001b[0m \u001b[39m# print(f\"dA shape: {dA.shape}\")\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=409'>410</a>\u001b[0m \u001b[39m# print(f\"sigmoid shape: {sigmoid.shape}\")\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=410'>411</a>\u001b[0m \u001b[39m# print(f\"(1 - sigmoid) shape: {(1 - sigmoid).shape}\")\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=412'>413</a>\u001b[0m dZ \u001b[39m=\u001b[39m dA \u001b[39m*\u001b[39;49m sigmoid \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m sigmoid)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/henryrscott/Desktop/Algos/final/final-nn/nn/sandbox.ipynb#W2sZmlsZQ%3D%3D?line=413'>414</a>\u001b[0m \u001b[39mreturn\u001b[39;00m dZ\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (32,64) (64,32) "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Get the data and labels\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Print the shape of the data and labels\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define architecture\n",
    "autoencoder_arch = [\n",
    "    {'input_dim': 64, 'output_dim': 16, 'activation': 'relu'},\n",
    "    {'input_dim': 16, 'output_dim': 64, 'activation': 'sigmoid'}\n",
    "]\n",
    "\n",
    "\n",
    "# Initialize and train the neural network\n",
    "nn = NeuralNetwork(nn_arch=[{'input_dim': 64, 'output_dim': 16, 'activation': 'relu'}, {'input_dim': 16, 'output_dim': 64, 'activation': 'sigmoid'}],\n",
    "                   lr=0.1, seed=42, batch_size=32, epochs=100, loss_function='mse')\n",
    "nn.fit(X_train, X_train, X_val, X_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
