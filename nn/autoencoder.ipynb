{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Union\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    This is a class that generates a fully-connected neural network.\n",
    "\n",
    "    Parameters:\n",
    "        nn_arch: List[Dict[str, float]]\n",
    "            A list of dictionaries describing the layers of the neural network.\n",
    "            e.g. [{'input_dim': 64, 'output_dim': 32, 'activation': 'relu'}, {'input_dim': 32, 'output_dim': 8, 'activation:': 'sigmoid'}]\n",
    "            will generate a two-layer deep network with an input dimension of 64, a 32 dimension hidden layer, and an 8 dimensional output.\n",
    "        lr: float\n",
    "            Learning rate (alpha).\n",
    "        seed: int\n",
    "            Random seed to ensure reproducibility.\n",
    "        batch_size: int\n",
    "            Size of mini-batches used for training.\n",
    "        epochs: int\n",
    "            Max number of epochs for training.\n",
    "        loss_function: str\n",
    "            Name of loss function.\n",
    "\n",
    "    Attributes:\n",
    "        arch: list of dicts\n",
    "            (see nn_arch above)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        nn_arch: List[Dict[str, Union[int, str]]],\n",
    "        lr: float,\n",
    "        seed: int,\n",
    "        batch_size: int,\n",
    "        epochs: int,\n",
    "        loss_function: str\n",
    "    ):\n",
    "\n",
    "        # Save architecture\n",
    "        self.arch = nn_arch\n",
    "\n",
    "        # Save hyperparameters\n",
    "        self._lr = lr\n",
    "        self._seed = seed\n",
    "        self._epochs = epochs\n",
    "        self._loss_func = loss_function\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        # Initialize the parameter dictionary for use in training\n",
    "        self._param_dict = self._init_params()\n",
    "\n",
    "    def _init_params(self) -> Dict[str, ArrayLike]:\n",
    "        \"\"\"\n",
    "        DO NOT MODIFY THIS METHOD! IT IS ALREADY COMPLETE!\n",
    "\n",
    "        This method generates the parameter matrices for all layers of\n",
    "        the neural network. This function returns the param_dict after\n",
    "        initialization.\n",
    "\n",
    "        Returns:\n",
    "            param_dict: Dict[str, ArrayLike]\n",
    "                Dictionary of parameters in neural network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Seed NumPy\n",
    "        np.random.seed(self._seed)\n",
    "\n",
    "        # Define parameter dictionary\n",
    "        param_dict = {}\n",
    "\n",
    "        # Initialize each layer's weight matrices (W) and bias matrices (b)\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            layer_idx = idx + 1\n",
    "            input_dim = layer['input_dim']\n",
    "            output_dim = layer['output_dim']\n",
    "            #print(f\"Initializing parameters for layer {layer_idx}: input_dim={input_dim}, output_dim={output_dim}\")\n",
    "            param_dict['W' + str(layer_idx)] = np.random.randn(output_dim, input_dim) * 0.1\n",
    "            param_dict['b' + str(layer_idx)] = np.random.randn(output_dim, 1) * 0.1\n",
    "            #print(f\"W{layer_idx} shape: {param_dict['W' + str(layer_idx)].shape}\")\n",
    "            #print(f\"b{layer_idx} shape: {param_dict['b' + str(layer_idx)].shape}\")\n",
    "        return param_dict\n",
    "\n",
    "    def _single_forward(self, A_prev: ArrayLike, W_curr: ArrayLike, b_curr: ArrayLike, activation: str) -> Tuple[ArrayLike, ArrayLike]:\n",
    "        \"\"\"\n",
    "        Perform a single forward pass for one layer of the neural network.\n",
    "\n",
    "        :param A_prev: Input activations from the previous layer.\n",
    "        :param W_curr: Weights for the current layer.\n",
    "        :param b_curr: Biases for the current layer.\n",
    "        :param activation: Activation function to be used for the current layer.\n",
    "        :return: A tuple containing the activations and pre-activations (Z) of the current layer.\n",
    "        \"\"\"\n",
    "        expected_input_dim = W_curr.shape[1]\n",
    "        assert A_prev.shape[0] == expected_input_dim, f\"Shape of A_prev {A_prev.shape} does not match expected input_dim {expected_input_dim}\"\n",
    "        Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "\n",
    "        if activation == \"sigmoid\":\n",
    "            A_curr = self._sigmoid(Z_curr)\n",
    "        elif activation == \"relu\":\n",
    "            A_curr = self._relu(Z_curr)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation}\")\n",
    "\n",
    "        return A_curr, Z_curr\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, X: ArrayLike) -> Tuple[ArrayLike, Dict[str, ArrayLike]]:\n",
    "        cache = {\"X\": X}\n",
    "        A_curr = X\n",
    "\n",
    "        for idx, layer in enumerate(self.arch):\n",
    "            A_prev = A_curr\n",
    "            W_curr = self._param_dict[\"W\" + str(idx + 1)]\n",
    "            b_curr = self._param_dict[\"b\" + str(idx + 1)]\n",
    "            activation = layer[\"activation\"] \n",
    "            A_curr, Z_curr = self._single_forward(A_prev, W_curr, b_curr, activation)\n",
    "            cache[\"A\" + str(idx + 1)] = A_curr\n",
    "            cache[\"Z\" + str(idx + 1)] = Z_curr\n",
    "\n",
    "        return A_curr.T, cache\n",
    "\n",
    "\n",
    "    def _single_backprop(\n",
    "    self,\n",
    "    dA_curr: ArrayLike,\n",
    "    W_curr: ArrayLike,\n",
    "    Z_curr: ArrayLike,\n",
    "    A_prev: ArrayLike,\n",
    "    activation_curr: str\n",
    "    ) -> Tuple[ArrayLike, ArrayLike, ArrayLike]:\n",
    "\n",
    "        if activation_curr == 'sigmoid':\n",
    "            dZ_curr = self._sigmoid_backprop(dA_curr, Z_curr)\n",
    "        elif activation_curr == 'relu':\n",
    "            dZ_curr = self._relu_backprop(dA_curr, Z_curr)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation_curr}\")\n",
    "\n",
    "        m = A_prev.shape[1]\n",
    "        dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "        db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "        dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "        return dA_prev, dW_curr, db_curr\n",
    "    \n",
    "    def backprop(self, y, y_hat, cache):\n",
    "        grad_dict = {}\n",
    "\n",
    "        if self._loss_func == \"binary_cross_entropy\":\n",
    "            dA_prev = self._binary_cross_entropy_backprop(y, y_hat)\n",
    "        elif self._loss_func == \"mean_squared_error\":\n",
    "            dA_prev = self._mean_squared_error_backprop(y, y_hat)\n",
    "\n",
    "        for layer_idx_prev, layer in reversed(list(enumerate(self.arch))):\n",
    "            layer_idx_curr = layer_idx_prev + 1\n",
    "            activation_curr = layer[\"activation\"]\n",
    "\n",
    "            dA_curr = dA_prev\n",
    "\n",
    "            if layer_idx_prev == 0:  # Use X as A0\n",
    "                A_prev = cache[\"X\"]\n",
    "            else:\n",
    "                A_prev = cache[\"A\" + str(layer_idx_prev)]\n",
    "            Z_curr = cache[\"Z\" + str(layer_idx_curr)]\n",
    "\n",
    "            W_curr = self._param_dict[\"W\" + str(layer_idx_curr)]\n",
    "\n",
    "            dA_prev, dW_curr, db_curr = self._single_backprop(\n",
    "                dA_curr, W_curr, Z_curr, A_prev, activation_curr\n",
    "            )\n",
    "\n",
    "            grad_dict[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "            grad_dict[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "\n",
    "        return grad_dict\n",
    "\n",
    "\n",
    "    def _update_params(self, grad_dict: Dict[str, ArrayLike]):\n",
    "        for layer_idx, layer in enumerate(self.arch):\n",
    "            layer_idx += 1\n",
    "            self._param_dict[f\"W{layer_idx}\"] -= self._lr * grad_dict[f\"dW{layer_idx}\"]\n",
    "            self._param_dict[f\"b{layer_idx}\"] -= self._lr * grad_dict[f\"db{layer_idx}\"]\n",
    "\n",
    "    def _transpose_input(self, X):\n",
    "        \"\"\"\n",
    "        Transpose the input data to match the internal representation of the\n",
    "        neural network.\n",
    "        \"\"\"\n",
    "        return X.T\n",
    "\n",
    "\n",
    "    def fit(self, X_train: ArrayLike, y_train: ArrayLike, X_val: ArrayLike, y_val: ArrayLike) -> Tuple[List[float], List[float]]:\n",
    "        np.random.seed(self._seed)\n",
    "        self._init_params()\n",
    "\n",
    "        train_loss_history = []\n",
    "        val_loss_history = []\n",
    "\n",
    "        for _ in range(self._epochs):\n",
    "            mini_batches = self._get_mini_batches(X_train, y_train)\n",
    "            \n",
    "            for X_mini_batch, y_mini_batch in mini_batches:\n",
    "                X_mini_batch = X_mini_batch.T\n",
    "                y_mini_batch = y_mini_batch.T\n",
    "                y_hat, cache = self.forward(X_mini_batch)\n",
    "                grad_dict = self.backprop(y_mini_batch, y_hat, cache)\n",
    "                self._update_params(grad_dict)\n",
    "\n",
    "            y_hat_train, _ = self.forward(X_train.T)\n",
    "            y_hat_val, _ = self.forward(X_val.T)\n",
    "\n",
    "            if self._loss_func == \"binary_cross_entropy\":\n",
    "                train_loss = self._binary_cross_entropy(y_train.T, y_hat_train.T)\n",
    "                val_loss = self._binary_cross_entropy(y_val.T, y_hat_val.T)\n",
    "            elif self._loss_func == \"mean_squared_error\":\n",
    "                train_loss = self._mean_squared_error(y_train.T, y_hat_train.T)\n",
    "                val_loss = self._mean_squared_error(y_val.T, y_hat_val.T)\n",
    "\n",
    "            train_loss_history.append(train_loss)\n",
    "            val_loss_history.append(val_loss)\n",
    "\n",
    "        return train_loss_history, val_loss_history\n",
    "\n",
    "    def _get_mini_batches(self, X, y):\n",
    "        \"\"\"\n",
    "        Generate mini-batches from the input data X and target data y.\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "        indices = np.arange(X.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        mini_batches = []\n",
    "        for idx in range(0, X.shape[0], self._batch_size):\n",
    "            batch_indices = indices[idx:idx+self._batch_size]\n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "            mini_batches.append((X_batch, y_batch))\n",
    "\n",
    "        return mini_batches\n",
    "\n",
    "    def predict(self, X: ArrayLike) -> ArrayLike:\n",
    "        y_hat, _ = self.forward(X.T)\n",
    "        return y_hat\n",
    "\n",
    "    \n",
    "    def _sigmoid(self, Z: ArrayLike) -> ArrayLike:\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "    def _sigmoid_backprop(self, dA: ArrayLike, Z: ArrayLike):\n",
    "        s = self._sigmoid(Z)\n",
    "        return dA * s * (1 - s)\n",
    "\n",
    "    def _relu(self, Z: ArrayLike) -> ArrayLike:\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def _relu_backprop(self, dA: ArrayLike, Z: ArrayLike) -> ArrayLike:\n",
    "        dZ = np.array(dA, copy=True)\n",
    "        dZ[Z <= 0] = 0\n",
    "        return dZ\n",
    "\n",
    "    def _binary_cross_entropy(self, y: ArrayLike, y_hat: ArrayLike) -> float:\n",
    "        m = y.shape[1]\n",
    "        loss = -(1 / m) * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "        return loss\n",
    "\n",
    "    def _binary_cross_entropy_backprop(self, y: ArrayLike, y_hat: ArrayLike) -> ArrayLike:\n",
    "        dA = - (np.divide(y, y_hat) - np.divide(1 - y, 1 - y_hat))\n",
    "        return dA\n",
    "\n",
    "    def _mean_squared_error(self, y: ArrayLike, y_hat: ArrayLike) -> float:\n",
    "        m = y.shape[1]\n",
    "        loss = (1 / (2 * m)) * np.sum(np.square(y_hat - y))\n",
    "        return loss\n",
    "\n",
    "    def _mean_squared_error_backprop(self, y: ArrayLike, y_hat: ArrayLike) -> ArrayLike:\n",
    "        m = y.shape[1]\n",
    "        dA = (2 / m) * (y_hat.T - y)\n",
    "        return dA\n",
    "    \n",
    "    def get_hyperparameters(self) -> dict:\n",
    "        \"\"\"\n",
    "        Get the current hyperparameters of the neural network.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing the hyperparameters.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'architecture': self.arch,\n",
    "            'learning_rate': self._lr,\n",
    "            'seed': self._seed,\n",
    "            'batch_size': self._batch_size,\n",
    "            'epochs': self._epochs,\n",
    "            'loss_function': self._loss_func\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reconstruction Error (Validation Set): 0.07386385613260882\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Normalize the data\n",
    "X /= 16\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Generate an instance of NeuralNetwork class with 64x16x64 autoencoder architecture\n",
    "nn_arch = [\n",
    "    {\"input_dim\": 64, \"output_dim\": 16, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 16, \"output_dim\": 64, \"activation\": \"sigmoid\"}\n",
    "]\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.01\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "loss_function = \"mean_squared_error\"\n",
    "\n",
    "# Create NeuralNetwork instance\n",
    "nn = NeuralNetwork(nn_arch, lr, seed, batch_size, epochs, loss_function)\n",
    "\n",
    "# Train the autoencoder\n",
    "train_loss, val_loss = nn.fit(X_train, X_train, X_val, X_val)\n",
    "\n",
    "# Plot training and validation loss by epoch\n",
    "plt.plot(train_loss, label=\"Training Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Quantify average reconstruction error over the validation set\n",
    "y_val_pred = nn.predict(X_val)\n",
    "reconstruction_error = np.mean((y_val_pred - X_val) ** 2)\n",
    "print(\"Average Reconstruction Error (Validation Set):\", reconstruction_error)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid search based hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'batch_size': 16, 'hidden_neurons': 8, 'learning_rate': 0.001}, Reconstruction Error: 0.14411911255165144\n",
      "Parameters: {'batch_size': 16, 'hidden_neurons': 8, 'learning_rate': 0.01}, Reconstruction Error: 0.06940218711489363\n",
      "Parameters: {'batch_size': 16, 'hidden_neurons': 8, 'learning_rate': 0.1}, Reconstruction Error: 0.029610887476430786\n",
      "Parameters: {'batch_size': 16, 'hidden_neurons': 16, 'learning_rate': 0.001}, Reconstruction Error: 0.11041965404774104\n",
      "Parameters: {'batch_size': 16, 'hidden_neurons': 16, 'learning_rate': 0.01}, Reconstruction Error: 0.062495984031591165\n",
      "Parameters: {'batch_size': 16, 'hidden_neurons': 16, 'learning_rate': 0.1}, Reconstruction Error: 0.018418971196789927\n",
      "Parameters: {'batch_size': 16, 'hidden_neurons': 32, 'learning_rate': 0.001}, Reconstruction Error: 0.086314712350473\n",
      "Parameters: {'batch_size': 16, 'hidden_neurons': 32, 'learning_rate': 0.01}, Reconstruction Error: 0.05329869970998774\n",
      "Parameters: {'batch_size': 16, 'hidden_neurons': 32, 'learning_rate': 0.1}, Reconstruction Error: 0.013262797432556001\n",
      "Parameters: {'batch_size': 32, 'hidden_neurons': 8, 'learning_rate': 0.001}, Reconstruction Error: 0.17524856432088398\n",
      "Parameters: {'batch_size': 32, 'hidden_neurons': 8, 'learning_rate': 0.01}, Reconstruction Error: 0.0757113866353713\n",
      "Parameters: {'batch_size': 32, 'hidden_neurons': 8, 'learning_rate': 0.1}, Reconstruction Error: 0.049806274405670356\n",
      "Parameters: {'batch_size': 32, 'hidden_neurons': 16, 'learning_rate': 0.001}, Reconstruction Error: 0.17228533733269152\n",
      "Parameters: {'batch_size': 32, 'hidden_neurons': 16, 'learning_rate': 0.01}, Reconstruction Error: 0.07386385613260882\n",
      "Parameters: {'batch_size': 32, 'hidden_neurons': 16, 'learning_rate': 0.1}, Reconstruction Error: 0.039458379497184976\n",
      "Parameters: {'batch_size': 32, 'hidden_neurons': 32, 'learning_rate': 0.001}, Reconstruction Error: 0.1565395724258041\n",
      "Parameters: {'batch_size': 32, 'hidden_neurons': 32, 'learning_rate': 0.01}, Reconstruction Error: 0.07121079175626896\n",
      "Parameters: {'batch_size': 32, 'hidden_neurons': 32, 'learning_rate': 0.1}, Reconstruction Error: 0.03238625923365087\n",
      "Parameters: {'batch_size': 64, 'hidden_neurons': 8, 'learning_rate': 0.001}, Reconstruction Error: 0.17925078924169624\n",
      "Parameters: {'batch_size': 64, 'hidden_neurons': 8, 'learning_rate': 0.01}, Reconstruction Error: 0.16322190426593314\n",
      "Parameters: {'batch_size': 64, 'hidden_neurons': 8, 'learning_rate': 0.1}, Reconstruction Error: 0.0716038618528962\n",
      "Parameters: {'batch_size': 64, 'hidden_neurons': 16, 'learning_rate': 0.001}, Reconstruction Error: 0.17969815704140402\n",
      "Parameters: {'batch_size': 64, 'hidden_neurons': 16, 'learning_rate': 0.01}, Reconstruction Error: 0.1447688756290948\n",
      "Parameters: {'batch_size': 64, 'hidden_neurons': 16, 'learning_rate': 0.1}, Reconstruction Error: 0.068176990801832\n",
      "Parameters: {'batch_size': 64, 'hidden_neurons': 32, 'learning_rate': 0.001}, Reconstruction Error: 0.17238256981724184\n",
      "Parameters: {'batch_size': 64, 'hidden_neurons': 32, 'learning_rate': 0.01}, Reconstruction Error: 0.10959596516282272\n",
      "Parameters: {'batch_size': 64, 'hidden_neurons': 32, 'learning_rate': 0.1}, Reconstruction Error: 0.062137163734893405\n",
      "Best Model Parameters: {'architecture': [{'input_dim': 64, 'output_dim': 32, 'activation': 'relu'}, {'input_dim': 32, 'output_dim': 64, 'activation': 'sigmoid'}], 'learning_rate': 0.1, 'seed': 42, 'batch_size': 16, 'epochs': 100, 'loss_function': 'mean_squared_error'}, Reconstruction Error: 0.013262797432556001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'hidden_neurons': [8, 16, 32]\n",
    "}\n",
    "\n",
    "# Create a ParameterGrid instance\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "# Store the best model and its reconstruction error\n",
    "best_model = None\n",
    "best_reconstruction_error = float('inf')\n",
    "\n",
    "# Perform Grid Search\n",
    "for params in grid:\n",
    "    # Update the architecture with the current hidden_neurons value\n",
    "    nn_arch[0]['output_dim'] = params['hidden_neurons']\n",
    "    nn_arch[1]['input_dim'] = params['hidden_neurons']\n",
    "\n",
    "    # Create NeuralNetwork instance with the current hyperparameters\n",
    "    nn = NeuralNetwork(nn_arch, params['learning_rate'], seed, params['batch_size'], epochs, loss_function)\n",
    "\n",
    "    # Train the autoencoder\n",
    "    train_loss, val_loss = nn.fit(X_train, X_train, X_val, X_val)\n",
    "\n",
    "    # Compute average reconstruction error over the validation set\n",
    "    y_val_pred = nn.predict(X_val)\n",
    "    reconstruction_error = np.mean((y_val_pred - X_val) ** 2)\n",
    "\n",
    "    # Update the best model if the current model has lower reconstruction error\n",
    "    if reconstruction_error < best_reconstruction_error:\n",
    "        best_reconstruction_error = reconstruction_error\n",
    "        best_model = nn\n",
    "\n",
    "    print(f\"Parameters: {params}, Reconstruction Error: {reconstruction_error}\")\n",
    "\n",
    "print(f\"Best Model Parameters: {best_model.get_hyperparameters()}, Reconstruction Error: {best_reconstruction_error}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search based hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'learning_rate': 0.001, 'batch_size': 16, 'epochs': 200, 'loss_function': 'mean_squared_error'}, Reconstruction Error: 0.07598955326821387\n",
      "Parameters: {'learning_rate': 0.1, 'batch_size': 64, 'epochs': 100, 'loss_function': 'mean_squared_error'}, Reconstruction Error: 0.068176990801832\n",
      "Parameters: {'learning_rate': 0.01, 'batch_size': 32, 'epochs': 100, 'loss_function': 'mean_squared_error'}, Reconstruction Error: 0.07386385613260882\n",
      "Parameters: {'learning_rate': 0.1, 'batch_size': 16, 'epochs': 100, 'loss_function': 'mean_squared_error'}, Reconstruction Error: 0.018418971196789927\n",
      "Parameters: {'learning_rate': 0.01, 'batch_size': 64, 'epochs': 50, 'loss_function': 'mean_squared_error'}, Reconstruction Error: 0.16839234815925916\n",
      "Parameters: {'learning_rate': 0.01, 'batch_size': 32, 'epochs': 50, 'loss_function': 'mean_squared_error'}, Reconstruction Error: 0.09183524208153625\n",
      "Parameters: {'learning_rate': 0.1, 'batch_size': 16, 'epochs': 100, 'loss_function': 'mean_squared_error'}, Reconstruction Error: 0.018418971196789927\n",
      "Parameters: {'learning_rate': 0.01, 'batch_size': 32, 'epochs': 50, 'loss_function': 'mean_squared_error'}, Reconstruction Error: 0.09183524208153625\n",
      "Parameters: {'learning_rate': 0.1, 'batch_size': 32, 'epochs': 100, 'loss_function': 'mean_squared_error'}, Reconstruction Error: 0.039458379497184976\n",
      "Parameters: {'learning_rate': 0.01, 'batch_size': 16, 'epochs': 100, 'loss_function': 'mean_squared_error'}, Reconstruction Error: 0.062495984031591165\n",
      "Best Model Parameters: {'architecture': [{'input_dim': 64, 'output_dim': 16, 'activation': 'relu'}, {'input_dim': 16, 'output_dim': 64, 'activation': 'sigmoid'}], 'learning_rate': 0.1, 'seed': 42, 'batch_size': 16, 'epochs': 100, 'loss_function': 'mean_squared_error'}, Reconstruction Error: 0.018418971196789927\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Normalize the data\n",
    "X /= 16\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fixed autoencoder architecture\n",
    "nn_arch = [\n",
    "    {\"input_dim\": 64, \"output_dim\": 16, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 16, \"output_dim\": 64, \"activation\": \"sigmoid\"}\n",
    "]\n",
    "\n",
    "# Random search hyperparameter ranges\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "batch_sizes = [16, 32, 64]\n",
    "epochs = [50, 100, 200]\n",
    "loss_functions = [\"mean_squared_error\"]\n",
    "n_search_iter = 10\n",
    "\n",
    "best_reconstruction_error = float('inf')\n",
    "best_model = None\n",
    "\n",
    "for _ in range(n_search_iter):\n",
    "    # Randomly select hyperparameters\n",
    "    lr = random.choice(learning_rates)\n",
    "    batch_size = random.choice(batch_sizes)\n",
    "    epoch = random.choice(epochs)\n",
    "    loss_function = random.choice(loss_functions)\n",
    "\n",
    "    # Create a NeuralNetwork instance\n",
    "    nn = NeuralNetwork(nn_arch, lr, seed, batch_size, epoch, loss_function)\n",
    "\n",
    "    # Train the autoencoder\n",
    "    train_loss, val_loss = nn.fit(X_train, X_train, X_val, X_val)\n",
    "\n",
    "    # Calculate average reconstruction error over the validation set\n",
    "    y_val_pred = nn.predict(X_val)\n",
    "    reconstruction_error = np.mean((y_val_pred - X_val) ** 2)\n",
    "\n",
    "    # Update best model and reconstruction error\n",
    "    if reconstruction_error < best_reconstruction_error:\n",
    "        best_reconstruction_error = reconstruction_error\n",
    "        best_model = nn\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': lr,\n",
    "        'batch_size': batch_size,\n",
    "        'epochs': epoch,\n",
    "        'loss_function': loss_function\n",
    "    }\n",
    "    print(f\"Parameters: {params}, Reconstruction Error: {reconstruction_error}\")\n",
    "\n",
    "print(f\"Best Model Parameters: {best_model.get_hyperparameters()}, Reconstruction Error: {best_reconstruction_error}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
